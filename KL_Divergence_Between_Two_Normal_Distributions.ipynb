{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task: Implement KL Divergence Between Two Normal Distributions\n",
        "\n",
        "Your task is to compute the Kullback-Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n",
        "\n",
        "Write a function kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q) that calculates the KL divergence between two normal distributions, where ( P \\sim N(\\mu_P, \\sigma_P^2) ) and ( Q \\sim N(\\mu_Q, \\sigma_Q^2) ).\n",
        "\n",
        "The function should return the KL divergence as a floating-point number.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input:\n",
        "\n",
        "mu_p = 0.0\n",
        "\n",
        "sigma_p = 1.0\n",
        "\n",
        "mu_q = 1.0\n",
        "\n",
        "sigma_q = 1.0\n",
        "\n",
        "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
        "Output:\n",
        "\n",
        "0.5\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "The KL divergence between the normal distributions ( P ) and ( Q ) with parameters ( \\mu_P = 0.0 ), ( \\sigma_P = 1.0 ) and ( \\mu_Q = 1.0 ), ( \\sigma_Q = 1.0 ) is 0.5.\n"
      ],
      "metadata": {
        "id": "F7_Syjrx6pn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def KL_divergence(mu_p,sigma_p,mu_q,sigma_q):\n",
        "  return math.log(sigma_p/sigma_q)+ (sigma_p**2+(mu_p-mu_q)**2)/(2*sigma_q**2)-0.5"
      ],
      "metadata": {
        "id": "z0gl17mo6-hg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(KL_divergence(0.0,1.0,1.0,1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW8ORFDhBn9f",
        "outputId": "5d923833-fd61-4b45-ae49-5c6bf4656f03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Kullback-Leibler Divergence (KL Divergence)\n",
        "KL Divergence is a key concept in probability theory and information theory, used to measure the difference between two probability distributions. It quantifies how much information is lost when one distribution is used to approximate another.\n",
        "\n",
        "What is KL Divergence?\n",
        "\n",
        "KL Divergence is defined as:\n",
        "\n",
        "DKL(P∥Q)=∑xP(x)log⁡(P(x)Q(x))DKL(P∥Q)=x∑P(x)log(Q(x)P(x))\n",
        "Where:\n",
        "\n",
        "1.\tP(x)P(x) is the true probability distribution.\n",
        "\n",
        "2.\tQ(x)Q(x) is the approximating probability distribution.\n",
        "\n",
        "3.\tThe sum is taken over all possible outcomes xx.\n",
        "\n",
        "Intuition Behind KL Divergence\n",
        "\n",
        "KL Divergence measures the \"extra\" number of bits required to code samples from P(x)P(x) using the distribution Q(x)Q(x), instead of using the true distribution P(x)P(x).\n",
        "\n",
        "•\tIf PP and QQ are identical, DKL(P∥Q)=0DKL(P∥Q)=0, meaning no extra bits are needed.\n",
        "\n",
        "•\tIf QQ is very different from PP, the divergence will be large, indicating a poor approximation.\n",
        "\n",
        "KL Divergence is always non-negative due to its relationship with the Kullback-Leibler inequality, which is a result of Gibbs' inequality.\n",
        "\n",
        "Key Properties\n",
        "\n",
        "1.\tAsymmetry: DKL(P∥Q)≠DKL(Q∥P)DKL(P∥Q)=DKL(Q∥P). That is, KL Divergence is not a true distance metric.\n",
        "\n",
        "2.\tNon-negativity: DKL(P∥Q)≥0DKL(P∥Q)≥0 for all probability distributions PP and QQ.\n",
        "\n",
        "3.\tApplicability: KL Divergence is used in various fields, including machine learning, data science, and natural language processing, to compare probability distributions or models.\n",
        "\n",
        "Example\n",
        "\n",
        "Consider two discrete probability distributions P(x)P(x) and Q(x)Q(x):\n",
        "P(x)=[0.4,0.6],Q(x)=[0.5,0.5]\n",
        "\n",
        "The KL Divergence between these two distributions is calculated as:\n",
        "\n",
        "DKL(P∥Q)=0.4log⁡(0.40.5)+0.6log⁡(0.60.5)DKL(P∥Q)=0.4log(0.50.4)+0.6log(0.50.6)\n",
        "This gives the divergence measure, quantifying how much information is lost when using Q(x)Q(x) to approximate P(x)P(x).\n",
        "\n",
        "KL Divergence plays an essential role in fields like machine learning, where it is used for tasks such as model evaluation, anomaly detection, and optimization.\n"
      ],
      "metadata": {
        "id": "lqf8D2mD6t3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SYz0LUV36V2"
      },
      "outputs": [],
      "source": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implement Gradient Descent Variants with MSE Loss\n",
        "In this problem, you need to implement a single function that can perform three variants of gradient descentâ€”Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descentâ€”using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n",
        "The function should return the final weights after performing the specified variant of gradient descent."
      ],
      "metadata": {
        "id": "gxgRprGtPqAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
        "  n_samples,n_features=X.shape\n",
        "  #all using MSE as loss function\n",
        "  #Batch Gradient Descent\n",
        "  for iteration in range(n_iterations):\n",
        "    if method=='batch':\n",
        "        y_pred=np.dot(X,weights)\n",
        "        error=y_pred-y\n",
        "        w_grad=(2/n_samples)*np.dot(X.T,error)\n",
        "        weights=weights-learning_rate*w_grad\n",
        "    #Mini-Batch Gradient Descent\n",
        "    elif method=='mini_batch':\n",
        "      for i in range(0,n_samples,batch_size):\n",
        "        x_batch=X[i:i+batch_size]\n",
        "        y_batch=y[i:i+batch_size]\n",
        "        y_pred=np.dot(x_batch,weights)\n",
        "        error=y_pred-y_batch\n",
        "        w_grad=(2/batch_size)*np.dot(x_batch.T,error)\n",
        "        weights=weights-learning_rate*w_grad\n",
        "    #Stochastic Gradient Descent (SGD)\n",
        "    elif method==\"stochastic\":\n",
        "      for i in range(n_samples):\n",
        "        x=X[i]\n",
        "        y_s=y[i]\n",
        "        y_pred=np.dot(x,weights)\n",
        "        error=y_pred-y_s\n",
        "        w_grad=2*np.dot(x.T,error)\n",
        "        weights=weights-learning_rate*w_grad\n",
        "\n",
        "  return weights"
      ],
      "metadata": {
        "id": "A-eUgjxAP719"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHnaqwfJ4_P3",
        "outputId": "755e6ad4-eb87-44ba-e930-2b8fd1660470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.01003164 0.97050576] [1.00000058 0.99999813] [1.0003804  0.99883421]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
        "y = np.array([2, 3, 4, 5])\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "n_iterations = 1000\n",
        "batch_size = 2\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.zeros(X.shape[1])\n",
        "\n",
        "# Test Batch Gradient Descent\n",
        "final_weights1 = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='batch')\n",
        "# Test Stochastic Gradient Descent\n",
        "final_weights2 = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='stochastic')\n",
        "# Test Mini-Batch Gradient Descent\n",
        "final_weights3 = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
        "print(final_weights1,final_weights2,final_weights3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdKq4IbeKIph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
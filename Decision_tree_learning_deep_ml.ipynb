{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "python function that implements the decsion tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. it should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as inout, and return a nested dictionary representing the decision tree."
      ],
      "metadata": {
        "id": "1y8unC1UkDdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our decision tree model needs to learn which features to take and the corresponding correct result values to optimally split the data\n",
        "how does the model decide the optimal split?\n",
        "with gini impurity , entropy or information gain because our purpose is to get pure leaf nodes so the split(based on features)will work that leads to pure leafs so the model will choose a split that maximizes the information gain and minimzes the gini or entropy"
      ],
      "metadata": {
        "id": "5IWdtq41ssKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_entropy(labels):\n",
        "    label_counts = Counter(labels)\n",
        "    total_count = len(labels)\n",
        "    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in label_counts.values())\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(examples, attr, target_attr):\n",
        "    total_entropy = calculate_entropy([example[target_attr] for example in examples])\n",
        "    values = set(example[attr] for example in examples)\n",
        "    attr_entropy = 0\n",
        "    for value in values:\n",
        "        value_subset = [example[target_attr] for example in examples if example[attr] == value]\n",
        "        value_entropy = calculate_entropy(value_subset)\n",
        "        attr_entropy += (len(value_subset) / len(examples)) * value_entropy\n",
        "    return total_entropy - attr_entropy\n",
        "\n",
        "def majority_class(examples, target_attr):\n",
        "    return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n",
        "\n",
        "def learn_decision_tree(examples, attributes, target_attr):\n",
        "    if not examples:\n",
        "        return 'No examples'\n",
        "    if all(example[target_attr] == examples[0][target_attr] for example in examples):\n",
        "        return examples[0][target_attr]\n",
        "    if not attributes:\n",
        "        return majority_class(examples, target_attr)\n",
        "\n",
        "    gains = {attr: calculate_information_gain(examples, attr, target_attr) for attr in attributes}\n",
        "    best_attr = max(gains, key=gains.get)\n",
        "    tree = {best_attr: {}}\n",
        "\n",
        "    for value in set(example[best_attr] for example in examples):\n",
        "        subset = [example for example in examples if example[best_attr] == value]\n",
        "        new_attributes = attributes.copy()\n",
        "        new_attributes.remove(best_attr)\n",
        "        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n",
        "        tree[best_attr][value] = subtree\n",
        "\n",
        "    return tree"
      ],
      "metadata": {
        "id": "HXB3w__JmjnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioPM5pc7j97Z"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n",
        "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n",
        "                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n",
        "                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n",
        "                ],\n",
        "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#output nested dictionary as decision tree\n",
        "{\n",
        "            'Outlook': {\n",
        "                'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n",
        "                'Overcast': 'Yes',\n",
        "                'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n",
        "            }\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "a43uQwj9lu5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasoning:\n",
        "Using the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure is able to classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'."
      ],
      "metadata": {
        "id": "d4Fy8mPOmIPy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PI4xZAhFmiww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
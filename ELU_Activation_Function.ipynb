{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implement the ELU (Exponential Linear Unit) activation function, which helps mitigate the limitations of ReLU by providing negative outputs for negative inputs. The function should compute the ELU activation value for a given input.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input:\n",
        "\n",
        "elu(-1)\n",
        "\n",
        "Output:\n",
        "\n",
        "-0.6321\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "For x = -1 and alpha = 1.0, the ELU activation is computed as α(ex−1)α(ex−1).\n",
        "Learn About topic\n"
      ],
      "metadata": {
        "id": "X7jS7m8poZP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KpzMBL8goNrO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def ELU(x,alpha=1):\n",
        "  if x<0:\n",
        "    return alpha*(np.exp(x)-1)\n",
        "  else:\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ELU(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im8O1JoCpDj8",
        "outputId": "73e4ff17-ec3d-4943-fa19-ed0f8c478e68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.6321205588285577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNOqNSjbpKWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}